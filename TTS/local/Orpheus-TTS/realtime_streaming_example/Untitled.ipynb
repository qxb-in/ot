{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9532a5a3-debb-48fb-8ac2-d5699ce60580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data5/anaconda3/envs/qxb_voice/lib/python3.10/site-packages/snac/snac.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(repo_id, \"pytorch_model.bin\"), map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-07 16:03:47 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 07-07 16:03:47 config.py:2444] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 07-07 16:03:52 config.py:549] This model supports multiple tasks: {'generate', 'reward', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 07-07 16:03:52 arg_utils.py:1187] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 07-07 16:03:52 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 07-07 16:03:52 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/data2/qinxb/model/Orpheus-TTS', speculative_config=None, tokenizer='/data2/qinxb/model/Orpheus-TTS', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/data2/qinxb/model/Orpheus-TTS, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 07-07 16:04:02 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 07-07 16:04:02 model_runner.py:1110] Starting to load model /data2/qinxb/model/Orpheus-TTS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d03513a48b456f852fb428c3f21c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-07 16:04:04 model_runner.py:1115] Loading model weights took 6.1801 GB\n",
      "INFO 07-07 16:04:05 worker.py:267] Memory profiling takes 0.83 seconds\n",
      "INFO 07-07 16:04:05 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.90) = 71.24GiB\n",
      "INFO 07-07 16:04:05 worker.py:267] model weights take 6.18GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.44GiB; the rest of the memory reserved for KV Cache is 63.52GiB.\n",
      "INFO 07-07 16:04:05 executor_base.py:111] # cuda blocks: 37168, # CPU blocks: 2340\n",
      "INFO 07-07 16:04:05 executor_base.py:116] Maximum concurrency for 131072 tokens per request: 4.54x\n",
      "INFO 07-07 16:04:08 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|███████████████████████████████████████████████████████████| 35/35 [00:11<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-07 16:04:19 model_runner.py:1562] Graph capturing finished in 12 secs, took 0.78 GiB\n",
      "INFO 07-07 16:04:19 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 15.30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from orpheus_tts import OrpheusModel\n",
    "import wave\n",
    "import time\n",
    "\n",
    "model = OrpheusModel(model_name =\"/data2/qinxb/model/Orpheus-TTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89e6fed2-477a-449f-8a9d-1cf5dbeca85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天天气怎么样啊，我想要出去旅行\n",
      "INFO 07-07 16:27:04 async_llm_engine.py:211] Added request req-001.\n",
      "INFO 07-07 16:27:04 metrics.py:455] Avg prompt throughput: 0.6 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 07-07 16:27:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 07-07 16:27:13 async_llm_engine.py:179] Finished request req-001.\n",
      "INFO 07-07 16:27:13 async_llm_engine.py:65] Engine is gracefully shutting down.\n",
      "It took 8.234334298991598 seconds to generate 5.21 seconds of audio\n"
     ]
    }
   ],
   "source": [
    "prompt = \"今天天气怎么样啊，我想要出去旅行\"\n",
    "\n",
    "start_time = time.monotonic()\n",
    "syn_tokens = model.generate_speech(\n",
    "   prompt=prompt,\n",
    "   voice=\"白芷\",\n",
    "   max_tokens=1000000,\n",
    "   )\n",
    "\n",
    "\n",
    "with wave.open(\"test.wav\", \"wb\") as wf:\n",
    "   wf.setnchannels(1)\n",
    "   wf.setsampwidth(2)\n",
    "   wf.setframerate(24000)\n",
    "\n",
    "   total_frames = 0\n",
    "   chunk_counter = 0\n",
    "   for audio_chunk in syn_tokens: # output streaming\n",
    "      chunk_counter += 1\n",
    "      frame_count = len(audio_chunk) // (wf.getsampwidth() * wf.getnchannels())\n",
    "      total_frames += frame_count\n",
    "      wf.writeframes(audio_chunk)\n",
    "   duration = total_frames / wf.getframerate()\n",
    "\n",
    "   end_time = time.monotonic()\n",
    "   print(f\"It took {end_time - start_time} seconds to generate {duration:.2f} seconds of audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1638028-8bb0-40c5-a231-23fac21ed979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
